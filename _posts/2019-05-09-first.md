---
published: false
title: "A more detailed solution to GDMC 2017"
categories:
  - video games
  - behavioral data
  - machine learning
---  
## Prediction engineering and other specific concepts that are not usually included and shown within a data science project


While data science has become essential to almost all industries,  in video games - reducing player churn is crucial to increase player engagement and game
monetization; besides, data science and predictive analytics show promises of transforming the video game industry as personalized user experience.
Game data sets are very had to find publicly available, especially coming from  massively multiplayer online role-playing game, not to mention any data science
solution related to a specific business problem. For this, the [Game Data Mining Competition (GDMC) 2017](https://cilab.sejong.ac.kr/gdmc2017/) organized by NCSoft provided a large data set from Blade and Soul
game log data. It consisted of one training dataset and two test datasets. Each of the datasets covered different periods of time and
had information about different users. The aim of this competition  was to predict churn and life expectancy during a certain prediction period.

![](https://github.com/DanielaLaura/DanielaLaura.github.io/blob/master/images/futureOfGaming1.png)
* [Future of Gaming](https://qz.com/is/what-happens-next-2/1438720/future-of-gaming/)


This article covers the concepts and full implementation as applied to predicting customer churn to the data set from GDMC 2017. 
The project [Jupyter Notebooks](https://github.com/DanielaLaura/dataproc_unittest/blob/master/Data/Data_processed/working%20on%20data%20formatting%20games.ipynb) and [Python files](https://github.com/DanielaLaura/dataproc_unittest) (also, organized as for a data production pipeline) for this  project are all available on GitHub.

When working with real-world data on a machine learning task, we define the problem, which means we have to develop our own labels - historical 
examples of what we want to predict, that is to train a supervised model. The idea of making our own labels may  seem foreign to data scientists  
who got started on Kaggle competitions or textbook datasets where the labels are already included, and this is the case of GDMC 2017. 
The concept behind prediction engineering-making labels to train a supervised machine learning model-is not a standardized process  and is done 
by data scientists on an as-needed basis. 

The inputs to prediction engineering are the parameters which define the prediction problem for the business requirement , and the historical dataset for finding 
examples of what we want to predict.
The output of prediction engineering is a label times table: a set of labels with negative and positive examples made from past data along with an associated 
cutoff time indicating when we have to stop using data to make features for that label. The labels are not complete without the cutoff time which represents when we have 
to stop using data to make features for a label. All the features for each label must use data from before this time to prevent the problem of data leakage. Cutoff times 
are a crucial part of building successful solutions to time-series problems that many companies do not account for. I have seen research paper describing the winning 
solution for this competition, accounting for the time after cuttoff as censored time. To be mentioned that using invalid data to make features leads to models that do 
well in development but fail in deployment.

### Dataset for Players Churn

Log data were provided in CSV format. Each player’s action log was stored in an individual 
file, with the user ID as the file name. In log files, where each row represents an observation and each column represents a variable
such as the time stamp, action type, and detailed information. Table 2 shows a sample log file.There were 82 action types, which represented 
various in-game player behaviors, such as logging in,leveling up, joining a guild, or buying an item.  

To be able to do an extensive data analysis,a unique data frame which contains all users and their simple statistics for action types is created  as follows:


<script src="https://gist.github.com/DanielaLaura/db1f80b4c22c3278975d5f6c130c5e4e.js" charset="utf-8">
</script>
<center>Data cleaning code.</center>

### Finding Historical Labels

The key to making prediction engineering adaptable to different problems is to follow a repeatable process for extracting training labels from a dataset. 
At a high-level this is outlined as follows:

1. Define positive and negative labels in terms of key business parameters
2. Search through past data for positive and negative examples
3. Make a table of cutoff times and associate each cutoff time with a label

For customer churn, the parameters are: 
- the prediction date (cutoff time): the point at which we make a prediction and when we stop using data to make features for the label
- number of days without playing before a user is considered a churn
- lead time: the number of days or months in the future we want to predict
- prediction window: the period of time we want to make predictions for
The following diagram shows each of these concepts while filling in the details with the problem definition we’ll work through.

![](https://github.com/DanielaLaura/DanielaLaura.github.io/blob/master/images/Diagram.png)
*Parameters defining the customer churn prediction problem.

Unlike the telecommunication services or other online subscriptions, for which a user churning can be easily defined and identified 
from the user unsubscribing, such isn’t the case for online game services. Online game players seldom delete their accounts
or unsubscribe although they have no intention of resuming playing the game. In this case, a user who does not play a game for
more than five weeks is defined as a churner.

### Labeling Implementation

To make labels, we develop one function (full code in the notebook):

[GIST]

The make_labels function then takes in the transactions for all customers along with the parameters and returns a table with the cutoff times and the label 
for every customer.

[GIST]

The Feature Engineering Process
Feature engineering, the second step in the machine learning pipeline, takes in the label times from the first step—prediction engineering and a raw dataset 
that needs to be refined. Feature engineering means building features for each label while filtering the data used for the feature based on the label’s cutoff 
time to make valid features. These features and labels are then passed to modeling where they will be used for training a machine learning algorithm.


Implementation of Modeling for Customer Churn

Although machine learning algorithms may sound technically complex, implementing them in Python is simple thanks to standard machine learning libraries like Scikit-Learn.
As a bit of practical advice, empirical results have shown that the choice of machine learning model and hyperparameters matters, but not as much as feature engineering.

In this project, I went with Scikit-Learn to rapidly implement a few models. To get the data ready for machine learning, we have to take some basic steps: missing value imputation,
encoding of categorical variables, and optionally feature selection if the input dimension is too large (see notebook for full details). Then, we can create a model with standard modeling syntax:

Metrics and Baseline Results
Before applying machine learning, it’s best to establish a naive baseline to determine if machine learning is actually helping. With a classification problem, this can be as simple as guessing the
majority label in the training data for all examples in the hold-out testing data. For the customer churn data, guessing every test label is not a churn yields an accuracy of 96.5%.
This high accuracy may sound impressive, but for an imbalanced classification problem-where one class is represented more than another-accuracy is not an adequate metric. Instead, we want to use recall,
precision, or the F1 score. 
Since this is a classification problem, for a machine learning baseline I tried a logistic regression which did not perform well. This indicates the problem is likely non-linear, so my second attempt
used a Random Forest Classifier with better results. The random forest is quick to train, relatively interpretable, highly accurate and is usually a solid model choice.

Each model was evaluated using about 30% of the data for holdout testing based on a time-series split. (This is crucial when evaluating a model in a time-series problem because it prevents training 
data leakage and should provide a good estimate of the actual model performance on new data.)

Aligning the Model with the Business Requirement
Even though the metrics for the ml models are better than with no machine learning, we want to optimize a model for a given metric(s) in line with the business need. In this example, we’ll focus on 
recall and precision. We will tune the model to achieve a certain recall by adjusting the threshold, the probability above which an observation is classified as positive-a churn.

Precision and Recall Tuning
There is a fundamental tradeoff in machine learning between recall and precision, which means we can increase one only at the cost of decreasing the other. For example, if we want to find every instance
of churn-a recall of 100%-then we would have to accept a low precision-many false positives. Conversely, if we limit the false positives by increasing the precision, then we will identify fewer of the actual 
churns lowering the recall.
The balance between these two is altered by adjusting the model’s threshold. We can visualize this in the model’s precision-recall curve.


Choosing the recall or precision lies in the business domain. It requires determining which is more costly, false positives-predicting a customer will churn when in fact they will not-or false negatives-predicting
a customer will not churn when in fact they will-and adjusting appropriately.

Model Validation
Once we have selected the threshold for classifying a churn, we can plot the confusion matrix from the holdout testing set to examine the predictions.
To make sure our model has solved the problem, we need to use the holdout results to calculate the return from implementing the model.

Validating Business Value
Using the model’s metrics on the hold-out testing set as an estimate of performance on new data, we can calculate the value of deploying this model before deploying it. Using the historical data, we first calculate 
the typical revenue lost to churn and then the reduced amount of revenue lost to churn with a model that achieves 75% recall and 8% precision.

As a final piece of model interpretation, we can look at the most important features to get a sense of the variables most relevant to the problem. The 10 most important variables from the random forest model are shown below


Making Predictions and Deployment
With our machine learning pipeline complete and the model validated, we are ready to make predictions of future customer churn. We don’t have live data for this project, but if we did, we could make predictions like the following:

In addition to making predictions each time we get new data, we’ll want to continue to validate our solution once it has been deployed. This means comparing model predictions to actual outcomes and looking at the data to check for 
concept drift. If performance decreases below the level of providing value, we can gather and train on more data, change the prediction problem, optimize the model settings, or adjust the tuned threshold.